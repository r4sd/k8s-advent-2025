# kube-prometheus-stack Helm values
# ç›£è¦–å°‚ç”¨ãƒãƒ¼ãƒ‰ (talos-np5-tbl) ã¸ã®é…ç½®è¨­å®šä»˜ã

# =============================================================================
# å…±é€šè¨­å®š: ç›£è¦–å°‚ç”¨ãƒãƒ¼ãƒ‰ã¸ã®é…ç½®
# =============================================================================
# å¯¾è±¡ãƒãƒ¼ãƒ‰: talos-np5-tbl (10.0.0.108)
# ãƒ©ãƒ™ãƒ«: node-role.kubernetes.io/monitoring=true
# Taint: dedicated=monitoring:NoSchedule

commonLabels:
  app.kubernetes.io/part-of: homelab-infra

# =============================================================================
# Prometheus
# =============================================================================
prometheus:
  prometheusSpec:
    # ç›£è¦–å°‚ç”¨ãƒãƒ¼ãƒ‰ã«é…ç½®
    nodeSelector:
      node-role.kubernetes.io/monitoring: "true"
    tolerations:
      - key: dedicated
        operator: Equal
        value: monitoring
        effect: NoSchedule

    # ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™ï¼ˆhomelabå‘ã‘ï¼‰
    resources:
      requests:
        cpu: 200m
        memory: 512Mi
      limits:
        cpu: 1000m
        memory: 2Gi

    # ãƒ‡ãƒ¼ã‚¿ä¿æŒæœŸé–“
    retention: 15d
    retentionSize: "10GB"

    # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸è¨­å®šï¼ˆStorageClass ãŒãªã„ãŸã‚ emptyDir ã‚’ä½¿ç”¨ï¼‰
    # Longhorn å°å…¥å¾Œã« PVC ã«å¤‰æ›´å¯èƒ½
    storageSpec: {}

    # å…¨Namespaceã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false

# =============================================================================
# Alertmanager
# =============================================================================
alertmanager:
  alertmanagerSpec:
    # ç›£è¦–å°‚ç”¨ãƒãƒ¼ãƒ‰ã«é…ç½®
    nodeSelector:
      node-role.kubernetes.io/monitoring: "true"
    tolerations:
      - key: dedicated
        operator: Equal
        value: monitoring
        effect: NoSchedule

    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 200m
        memory: 256Mi

  # AlertManagerè¨­å®š
  config:
    global:
      resolve_timeout: 5m

    route:
      receiver: discord
      group_by: ['alertname', 'namespace']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 4h
      routes:
        # Critical ã¯å³åº§ã«é€šçŸ¥
        - match:
            severity: critical
          receiver: discord
          group_wait: 10s
          repeat_interval: 1h
        # Warning ã¯é€šå¸¸é€šçŸ¥
        - match:
            severity: warning
          receiver: discord

    receivers:
      - name: discord
        discord_configs:
          # TODO: ç’°å¢ƒã«åˆã‚ã›ã¦å¤‰æ›´ã—ã¦ãã ã•ã„
          - webhook_url: "https://discord.com/api/webhooks/YOUR_WEBHOOK_ID/YOUR_WEBHOOK_TOKEN"
            send_resolved: true
            title: '{{ if eq .Status "firing" }}ğŸ”¥ Alert{{ else }}âœ… Resolved{{ end }}'
            message: |
              **{{ .CommonAnnotations.summary }}**
              {{ .CommonAnnotations.description }}
              Severity: {{ .CommonLabels.severity }}

    inhibit_rules:
      - source_match:
          severity: critical
        target_match:
          severity: warning
        equal: ['alertname', 'namespace']

# =============================================================================
# Grafana
# =============================================================================
grafana:
  # ç›£è¦–å°‚ç”¨ãƒãƒ¼ãƒ‰ã«é…ç½®
  nodeSelector:
    node-role.kubernetes.io/monitoring: "true"
  tolerations:
    - key: dedicated
      operator: Equal
      value: monitoring
      effect: NoSchedule

  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 512Mi

  # ç®¡ç†è€…ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ï¼ˆæœ¬ç•ªã§ã¯ Secret åŒ–æ¨å¥¨ï¼‰
  adminPassword: "admin"  # TODO: å¤‰æ›´ã—ã¦ãã ã•ã„

  # Ingressè¨­å®š
  ingress:
    enabled: true
    ingressClassName: nginx
    annotations:
      cert-manager.io/cluster-issuer: homelab-ca-issuer
    hosts:
      - grafana.homelab.local
    # TLSè¨­å®šï¼ˆcert-manager ã§è‡ªå‹•ç™ºè¡Œï¼‰
    tls:
      - secretName: grafana-tls
        hosts:
          - grafana.homelab.local

  # è¿½åŠ ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
  additionalDataSources: []

  # ã‚µã‚¤ãƒ‰ã‚«ãƒ¼è¨­å®šï¼ˆConfigMap ã‹ã‚‰ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è‡ªå‹•èª­ã¿è¾¼ã¿ï¼‰
  sidecar:
    dashboards:
      enabled: true
      label: grafana_dashboard
      searchNamespace: ALL
    datasources:
      enabled: true
      label: grafana_datasource

# =============================================================================
# Node Exporterï¼ˆå…¨ãƒãƒ¼ãƒ‰ã§å®Ÿè¡Œ - DaemonSetï¼‰
# =============================================================================
nodeExporter:
  enabled: true
  # DaemonSetãªã®ã§å…¨ãƒãƒ¼ãƒ‰ã§å®Ÿè¡Œï¼ˆtolerationsä¸è¦ï¼‰

# =============================================================================
# kube-state-metrics
# =============================================================================
kubeStateMetrics:
  enabled: true

# =============================================================================
# Prometheus Operator
# =============================================================================
prometheusOperator:
  # ç›£è¦–å°‚ç”¨ãƒãƒ¼ãƒ‰ã«é…ç½®
  nodeSelector:
    node-role.kubernetes.io/monitoring: "true"
  tolerations:
    - key: dedicated
      operator: Equal
      value: monitoring
      effect: NoSchedule

  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi

# =============================================================================
# è¿½åŠ ã®ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«
# =============================================================================
additionalPrometheusRulesMap:
  homelab-rules:
    groups:
      - name: homelab-node-alerts
        rules:
          # ãƒãƒ¼ãƒ‰ãƒ€ã‚¦ãƒ³æ¤œçŸ¥
          - alert: NodeDown
            expr: up{job="node-exporter"} == 0
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: "Node {{ $labels.instance }} is down"
              description: "Node {{ $labels.instance }} has been down for more than 2 minutes."

          # CPUä½¿ç”¨ç‡ > 85%ï¼ˆãƒ›ãƒ¼ãƒ ãƒ©ãƒœå‘ã‘ç·©å’Œè¨­å®šï¼‰
          - alert: HighCpuUsage
            expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 85
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "High CPU usage on {{ $labels.instance }}"
              description: "CPU usage is above 85% (current: {{ $value }}%)"

          # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ > 85%
          - alert: HighMemoryUsage
            expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High memory usage on {{ $labels.instance }}"
              description: "Memory usage is above 85% (current: {{ $value }}%)"

          # ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨ç‡ > 85%ï¼ˆãƒ›ãƒ¼ãƒ ãƒ©ãƒœå‘ã‘ç·©å’Œè¨­å®šï¼‰
          - alert: HighDiskUsage
            expr: (1 - (node_filesystem_avail_bytes{fstype!="tmpfs"} / node_filesystem_size_bytes{fstype!="tmpfs"})) * 100 > 85
            for: 15m
            labels:
              severity: warning
            annotations:
              summary: "High disk usage on {{ $labels.instance }}"
              description: "Disk usage is above 85% on {{ $labels.mountpoint }} (current: {{ $value }}%)"

      - name: homelab-kubernetes-alerts
        rules:
          # Pod CrashLoopBackOff
          - alert: PodCrashLoopBackOff
            expr: increase(kube_pod_container_status_restarts_total[1h]) > 5
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is in CrashLoopBackOff"
              description: "Pod has restarted {{ $value }} times in the last hour"

          # Pod Pending çŠ¶æ…‹ãŒç¶šãï¼ˆ10åˆ†ä»¥ä¸Šï¼‰
          - alert: PodPendingTooLong
            expr: kube_pod_status_phase{phase="Pending"} == 1
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is Pending"
              description: "Pod has been in Pending state for more than 10 minutes"

      - name: homelab-lb-alerts
        rules:
          # MetalLB Speaker Podåœæ­¢ï¼ˆ4å°ä¸­2å°ä»¥ä¸Šåœæ­¢ã§Criticalï¼‰
          - alert: MetalLBSpeakerDown
            expr: count(up{job="metallb-speaker"} == 1) < 3
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: "MetalLB Speaker pods are down"
              description: "Only {{ $value }} MetalLB Speaker pods are running (expected 4)"

          # MetalLB IP Poolæ¯æ¸‡è­¦å‘Šï¼ˆæ®‹ã‚Š2ä»¥ä¸‹ï¼‰
          - alert: MetalLBIPPoolLow
            expr: metallb_allocator_addresses_available <= 2
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "MetalLB IP pool running low"
              description: "Only {{ $value }} IP addresses remaining in pool"

          # MetalLB IP Poolæ¯æ¸‡ï¼ˆæ®‹ã‚Š0ï¼‰
          - alert: MetalLBIPPoolExhausted
            expr: metallb_allocator_addresses_available == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "MetalLB IP pool exhausted"
              description: "No IP addresses available in pool"

          # Ingress Controlleråœæ­¢
          - alert: IngressControllerDown
            expr: kube_deployment_status_replicas_ready{deployment="ingress-nginx-controller"} == 0
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: "Ingress Controller is down"
              description: "No ready replicas for ingress-nginx-controller"

          # Ingress 5xx ã‚¨ãƒ©ãƒ¼ç‡ï¼ˆè­¦å‘Šï¼‰
          - alert: IngressHighErrorRate
            expr: |
              sum(rate(nginx_ingress_controller_requests{status=~"5.."}[5m]))
              / sum(rate(nginx_ingress_controller_requests[5m])) * 100 > 5
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High 5xx error rate on Ingress"
              description: "5xx error rate is {{ $value }}% (threshold: 5%)"

          # Ingress 5xx ã‚¨ãƒ©ãƒ¼ç‡ï¼ˆCriticalï¼‰
          - alert: IngressCriticalErrorRate
            expr: |
              sum(rate(nginx_ingress_controller_requests{status=~"5.."}[5m]))
              / sum(rate(nginx_ingress_controller_requests[5m])) * 100 > 20
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: "Critical 5xx error rate on Ingress"
              description: "5xx error rate is {{ $value }}% (threshold: 20%)"
